# Human-Centered, Intention-Aware Explanations for Code Generation

This repository contains a curated set of prompt–response experiments focused on understanding how current code generation models handle:

- Intent ambiguity
- Explanation clarity
- Scope control
- Human-aligned reasoning

## 📁 Structure
- `prompts/` — categorized prompts for testing
- `responses/` — raw and annotated model outputs
- `evaluation/` — tracking sheet and scoring rubric
- `notebooks/` — optional analysis tools

## ✅ Goals
- Identify gaps in current models' ability to infer user intent
- Propose metrics for intent-alignment and human-centered explanations
- Showcase improvement opportunities in explainable code generation
